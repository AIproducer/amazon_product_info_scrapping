{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the webdriver\n",
    "driver=webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the scraped data\n",
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the target url\n",
    "url = 'https://www.amazon.com/s?k=laptop&i=computers&rh=n%3A565108%2Cp_n_feature_thirty-one_browse-bin%3A23716057011%2Cp_89%3AHP%7CHarry+Potter%7CLenovo%7Cacer&dc&ds=v1%3A2kz6M%2BtbRmyqkW0F2dG05lGYrRUYwEdWvKVS%2Fac1Tmk&qid=1689472069&rnid=2528832011&ref=sr_nr_p_89_4'\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get url links of products\n",
    "product_links = driver.find_elements(By.XPATH, \"//a[@class='a-link-normal s-no-outline']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in product_links:\n",
    "    link_url = link.get_attribute(\"href\")\n",
    "    print(link_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the sub target url\n",
    "driver.get(link_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the table element to load (adjust the time if needed)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page source\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table element containing the information\n",
    "table = soup.find(\"table\", class_=\"a-normal a-spacing-micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows in the table\n",
    "rows = table.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the scraped data\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    label_element = row.find(\"span\", class_=\"a-size-base a-text-bold\")\n",
    "    value_element = row.find(\"span\", class_=\"a-size-base po-break-word\")\n",
    "    if label_element and value_element:\n",
    "        label = label_element.get_text(strip=True)\n",
    "        value = value_element.get_text(strip=True)\n",
    "        data[label] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_rating_element = soup.find(\"span\", class_=\"reviewCountTextLinkedHistogram noUnderline\")\n",
    "\n",
    "# Extract the label and value\n",
    "label = span_rating_element.get(\"class\")[0]  # Use the first class as the label\n",
    "value = span_rating_element.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[label] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the span element with class \"a-price-whole\"\n",
    "span_element = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "\n",
    "# Extract the label and value\n",
    "label = span_element.get(\"class\")[0]  # Use the class as the label\n",
    "value = span_element.get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[label] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the scraped data to the list\n",
    "data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Hard Disk Size': 'Hard Disk', 'Ram Memory Installed Size': 'RAM', 'Operating System': 'os', 'Graphics Card Description': 'Graphics', 'reviewCountTextLinkedHistogram': 'Rating', 'a-price-whole': 'Price' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "#setup the webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Base URL for the product search\n",
    "base_url = 'https://www.amazon.com/s?k=laptop&i=computers&rh=n%3A565108%2Cp_n_feature_thirty-one_browse-bin%3A23716057011%2Cp_89%3AHP%7CHarry+Potter%7CLenovo%7Cacer&dc&page={}&qid=1689481617&rnid=2528832011&ref=sr_pg_{}'\n",
    "\n",
    "# Create a list to store the scraped data\n",
    "data_list = []\n",
    "\n",
    "# Define the range of pages to scrape\n",
    "start_page = 1\n",
    "end_page = 20\n",
    "\n",
    "# Scrape data from each page\n",
    "for page in range(start_page, end_page + 1):\n",
    "    # Generate the URL for the current page\n",
    "    url = base_url.format(page, page)\n",
    "    driver.get(url)\n",
    "\n",
    "    #get url links of products\n",
    "    product_links = driver.find_elements(By.XPATH, \"//a[@class='a-link-normal s-no-outline']\")\n",
    "\n",
    "    # Get the href attribute values of the product links\n",
    "    link_urls = [link.get_attribute(\"href\") for link in product_links]\n",
    "\n",
    "    # Scrape data from each product link\n",
    "    for link_url in link_urls:\n",
    "        driver.get(link_url)\n",
    "\n",
    "        # Wait for the table element to load (adjust the time if needed)\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Get the page source\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Create BeautifulSoup object\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Find the table element containing the information\n",
    "        table = soup.find(\"table\", class_=\"a-normal a-spacing-micro\")\n",
    "\n",
    "        # Find all rows in the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Create a dictionary to store the scraped data\n",
    "        data = {}\n",
    "\n",
    "        for row in rows:\n",
    "            label_element = row.find(\"span\", class_=\"a-size-base a-text-bold\")\n",
    "            value_element = row.find(\"span\", class_=\"a-size-base po-break-word\")\n",
    "            if label_element and value_element:\n",
    "                label = label_element.get_text(strip=True)\n",
    "                value = value_element.get_text(strip=True)\n",
    "                data[label] = value\n",
    "\n",
    "        # Check if the span_rating_element exists before attempting to extract its attributes\n",
    "        span_rating_element = soup.find(\"span\", class_=\"reviewCountTextLinkedHistogram noUnderline\")\n",
    "        if span_rating_element:\n",
    "            label = span_rating_element.get(\"class\")[0]  # Use the first class as the label\n",
    "            value = span_rating_element.get_text(strip=True)\n",
    "            data[label] = value\n",
    "\n",
    "        # Find the span element with class \"a-price-whole\"\n",
    "        span_element = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "        if span_element:\n",
    "            label = span_element.get(\"class\")[0]  # Use the class as the label\n",
    "            value = span_element.get_text(strip=True)\n",
    "            data[label] = value\n",
    "\n",
    "        # Append the scraped data to the list\n",
    "        data_list.append(data)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Hard Disk Size': 'Hard Disk', 'Ram Memory Installed Size': 'RAM', 'Operating System': 'os', 'Graphics Card Description': 'Graphics', 'reviewCountTextLinkedHistogram': 'Rating', 'a-price-whole': 'Price' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in csv format\n",
    "df.to_csv('amazon_laptop_product.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in csv format\n",
    "df.to_excel('amazon_laptop_product.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
